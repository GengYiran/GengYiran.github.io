<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yiran Geng   |  ËÄøÈÄ∏ÁÑ∂</title>
  <meta name="author" content="Yiran Geng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/icon/icon.jpg">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a4292ca8f2fe5fd7dc6dfd78cc894aab";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:68%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yiran Geng &nbsp | &nbsp ËÄøÈÄ∏ÁÑ∂</name>
              </p>
              <p>
              I am a senior undergraduate student in the <a href="https://cfcs.pku.edu.cn/research/turing_program/introduction1/index.htm" target="_blank">Turing class</a> at the <a href="https://eecs.pku.edu.cn/" target="_blank">School of EECS</a>, <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a> 
              , co-advised by Prof. <a href="https://zsdonghao.github.io" target="_blank">Hao Dong</a> 
              and Prof. <a href="https://www.yangyaodong.com/" target="_blank">Yaodong Yang</a>.   
              <!-- In addition, I am privileged to work closely with Prof. <a href="https://hughw19.github.io/" target="_blank">He Wang</a> and <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>. 
              I am currently a visiting researcher at <a href="https://www.mit.edu/" target="_blank">MIT</a>, advised by Prof. <a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a> and Prof. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>. -->
              It is worth mentioning that I have a twin brother named <a class="a1" href="https://geng-haoran.github.io/" target="_blank">Haoran Geng</a>, as our experiences have shown that we are frequently mistaken for one anotherü§£.
            </p>
              <p style="text-align:center">
                <a href="mailto:gyr@stu.pku.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q22ys2QAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/GengYiran/">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/geng_yiran/">Twitter</a>&nbsp/&nbsp
                <a href="images/my/WeChat.jpeg">WeChat</a>&nbsp/&nbsp
                <a href="https://b23.tv/8BLqaqH">Bilibili</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/my/photo_life.jpg"><img style="width:85%;max-width:85%" alt="profile photo" src="images/my/photo_life.jpg" class="hoverZoomLink"></a>
              <!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgengyiran.github.io&count_bg=%23E79717E9&title_bg=%23575454&icon=&icon_color=%23222121&title=hits&edge_flat=true"/></a> -->
              <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgengyiran.github.io&count_bg=%23E79717E9&title_bg=%23575454&icon=&icon_color=%23222121&title=visits&edge_flat=true"/></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <heading>Research Overview</heading>
            <br>
          </tr>
          <p>
            My research focuses on reinforcement learning, robotics, and computer vision. I also have a broad curiosity about computer science, mathematics, physics and art.
            <br>
            My current research interests are centered on facilitating the acquisition of <strong>generalizable</strong>, <strong>reliable</strong>, 
            and <strong>complex</strong> manipulation skills by intelligent systems, while simultaneously ensuring 
            their <strong>real-world deployable</strong>. At the same time, I am actively investigating 
            <strong>learning from interaction</strong>, which has the unique potential to significantly reduce the need 
            for perceptual data annotation and enable scalable training for large models.
          </p>
          <br>
          <tr>
                <img src='images/my/research_overview.png' width="770">
          </tr>
          <br>
          <br>

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications</heading>
          <br>
          

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/ManipLLM.png' width="190"></div>
                  <br>
                  <img src='images/ManipLLM.png' width="190">
                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.16217">
                <papertitle>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</papertitle>
              </a>
              <br>
              <a:focus>Xiaoqi Li</a:focus>, 
              <a:focus>Mingxu Zhang</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a>,
              <a:focus>Haoran Geng</a:focus>,
              <a:focus>Yuxing Long</a:focus>,
              <a:focus>Yan Shen</a:focus>,
              <a:focus>Renrui Zhang</a:focus>,
              <a:focus>Jiaming Liu</a:focus>,
              <a:focus>Hao Dong</a:focus>
              <br>
              <a href="https://sites.google.com/view/manipllm">Website</a>
              /
              <a href="https://arxiv.org/abs/2312.16217">arXiv</a>
              /
              <a href="https://mp.weixin.qq.com/s/YTvN6VDmP9aSxvI5M3YVCg">ÈáèÂ≠ê‰Ωç</a>
              <br>
              <em><strong>CVPR 2024</strong>, Accepted</em>
              <p></p>
              <p>
              We introduce an innovative approach that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs).
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/multigrasp.gif' width="190"></div>
                  <br>
                  <img src='images/multigrasp.gif' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MultiGrasp/MultiGrasp?style=flat-square&color=yellow&label=stars&logo=GitHub"> 

                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2310.15599">
                <papertitle>Grasp Multiple Objects with One Hand</papertitle>
              </a>
              <br>
              <a:focus>Yuyang Li</a:focus>, 
              <a:focus>Bo Liu</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a>,
              <a:focus>Puhao Li</a:focus>,
              <a:focus>Yaodong Yang</a:focus>,
              <a:focus>Yixin Zhu</a:focus>,
              <a:focus>Tengyu Liu</a:focus>,
              <a:focus>Siyuan Huang</a:focus>
              <br>
              <a href="https://multigrasp.github.io/">Website</a>
              /
              <a href="https://arxiv.org/pdf/2310.15599">arXiv</a>
              /
              <a href="MultiGrasp/MultiGrasp">Code</a>
              /
              <a href="MultiGrasp/MultiGrasp">DataSet</a>
              <br>
              <em><strong>RA-L 2024</strong>, Accepted</em>
              <p></p>
              <p>
                We propose MultiGrasp, a two-stage framework for simultaneous multi-object grasping with multi-finger dexterous hands.              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/RGBManip.png' width="190"></div>
                  <br>
                  <img src='images/RGBManip.png' width="190">

                </div>
                
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.03478">
                <papertitle>RGBManip: Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</papertitle>
              </a>
              <br>
              <a:focus>Boshi An*</a:focus>, 
              <a:focus><strong>Yiran Geng*</strong></a>,
              <a:focus>Kai Chen*</a:focus>,
              <a:focus>Xiaoqi Li</a:focus>,
              <a:focus>Qi Dou</a:focus>,
              <a:focus>Hao Dong</a:focus>
              <br>
              <a href="https://rgbmanip.github.io/">Website</a>
              /
              <a href="https://arxiv.org/abs/2310.03478">arXiv</a>
              /
              <a href="https://mp.weixin.qq.com/s/JIvXFT7oZkmHIygbbNPmRA">Media (CFCS)</a>
              
              <br>
              <em><strong>ICRA 2024</strong>, Accepted</em>
              <p></p>
              <p>
                We propose an image-only robotic manipulation framework with ability to actively perceive object from multiple perspectives during the manipulation process.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/safety-gym.png' width="190"></div>
                  <img src='images/safety-gym.png' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-Alignment/Safe-Policy-Optimization?style=flat-square&color=yellow&label=SafePO stars&logo=GitHub"> 
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-Alignment/safety-gymnasium?style=flat-square&color=yellow&label=Gymnasium stars&logo=GitHub"> 
                </div>
                
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://github.com/PKU-Alignment/safety-gymnasium">
                <papertitle>Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark</papertitle>
              </a>
              <br>
              <a:focus>Jiaming Ji*</a:focus>, 
              <a:focus>Borong Zhang*</a>,
              <a:focus>Jiayi Zhou*</a:focus>,
              <a:focus>Xuehai Pan</a:focus>,
              <a:focus>Weidong Huang</a:focus>,
              <a:focus>Ruiyang Sun</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Juntai Dai</a:focus>,
              <a:focus>Yaodong Yang</a:focus>
              <br>
              <a href="https://sites.google.com/view/safety-gymnasium">Website</a>
              /
              <a href="https://github.com/PKU-Alignment/safety-gymnasium">Code (Safety Gymnasium)</a>
              /
              <a href="https://github.com/PKU-Alignment/Safe-Policy-Optimization">Code (SafePO)</a>
              /
              <a href="https://www.safety-gymnasium.com/en/latest/">Documentation (Safety Gymnasium)</a>
              /
              <a href="https://safe-policy-optimization.readthedocs.io/en/latest/">Documentation (SafePO)</a>
              <br>
              
              <em><strong>NeurIPS 2023</strong>, Accepted</em>
              <p></p>
              <p>
                We present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios and a library of algorithms named Safe Policy Optimization (SafePO).
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/2022dexterous_benchark.gif' width="190"></div>
                  <br>
                <img src='images/2022dexterous_benchark.gif' width="190">
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-MARL/DexterousHands?style=flat-square&logo=GitHub&color=yellow">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2206.08686">
                <papertitle>Bi-DexHands: Towards Human-Level Bimanual Dexterous Manipulation</papertitle>
              </a>
              <br>
              <a:focus href="https://cypypccpy.github.io/">Yuanpei Chen</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a:focus>, 
              <a:focus>Fangwei Zhong</a:focus>,
              <a:focus href="https://github.com/waterhorse1">Jiaming Ji</a:focus>,
              <a:focus href="https://github.com/jiechuanjiang">Jiechuang Jiang</a:focus>,
              <a:focus href="https://z0ngqing.github.io">Zongqing Lu</a:focus>,
              <a:focus href="https://zsdonghao.github.io">Hao Dong</a:focus>,
              <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10343126">Paper (Long Version)</a>
              /
              <a href="https://arXiv.org/abs/2206.08686">Paper (Short Version)</a>
              /
              <a href="https://bi-dexhands.ai/">Project Page</a>
              /
              <a href="https://github.com/PKU-MARL/DexterousHands">Code</a>  
              <br>
              <em><strong>T-PAMI 2023</strong>, Accepted</em>
              <br>
              <em><strong>NeurIPS 2022</strong>, Accepted</em>
              <p>
                We propose a bimanual dexterous manipulation benchmark according to literature from cognitive science for comprehensive reinforcement learning research.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/omnisafe.png' width="190"></div>
                  <br>
                <img src='images/omnisafe.png' width="190">
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-Alignment/omnisafe?style=flat-square&logo=GitHub&color=yellow">
              </div>
              

              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
  
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2305.09304">
                <papertitle>OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research</papertitle>
              </a>
              <br>
              <a:focus>Jiaming Ji*</a>,
              <a:focus>Jiayi Zhou*</a:focus>, 
              <a:focus>Borong Zhang*</a:focus>,
              <a:focus>Juntao Dai</a:focus>,
              <a:focus>Xuehai Pan</a:focus>,
              <a:focus>Ruiyang Sun</a:focus>,
              <a:focus>Weidong Huang</a:focus>.
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Mickel Liu</a:focus>,
              <a:focus>Yaodong Yang</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2305.09304">arXiv</a>
              /
              <a href="https://github.com/PKU-Alignment/omnisafe">Code</a>
              /
              <a href="https://omnisafe.readthedocs.io/">Documentation</a>
              /
              <a href="images/omnisafe_wechat.jpg">Community</a>
              <br>
              <em>arXiv 2023</em>
              <p></p>
              <p>
                OmniSafe is an infrastructural framework designed to accelerate safe reinforcement learning (RL) research. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/ReDMan.png' width="190"></div>
                  <br>
                <img src='images/ReDMan.png' width="190">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
  
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="pdf/RedMan.pdf">
                <papertitle>ReDMan: Reliable Dexterous Manipulation with Safe Reinforcement Learning</papertitle>
              </a>
              <br>
              <a:focus><strong>Yiran Geng*</strong></a>,
              <a:focus>Jiaming Ji*</a:focus>, 
              <a:focus>Yuanpei Chen*</a:focus>,
              <a:focus>Haoran Geng</a:focus>,
              <a:focus>Fangwei Zhong</a:focus>,
              <a:focus>Yaodong Yang</a:focus>
              <br>
              <!-- <a href="pdf/RedMan.pdf">Paper</a> -->
              <!-- / -->
              <a href="https://github.com/PKU-Alignment/ReDMan">Code</a>
              <br>
              <em><strong>Machine Learning (Journal)</strong>, Minor Revision</em>
              <p></p>
              <p>
                We introduce ReDMan, an open-source simulation platform that provides a standardized implementation of safe RL algorithms for Reliable Dexterous Manipulation. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/PartManip.png' width="194"></div>
                <img src='images/PartManip.png' width="194">
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-EPIC/PartManip?style=flat-square&logo=GitHub&color=yellow">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/pgvp/" target="_blank">
                <papertitle>PartManip: Learning Part-based Cross-Category Object Manipulation Policy from Point Cloud Observations</papertitle>
              </a>
              <br>
              <a:focus>Haoran Geng*</a:focus>,
              <a:focus>Ziming Li*</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Jiayi Chen</a:focus>,
              <a:focus>Hao Dong</a:focus>,
              <a:focus>He Wang</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2303.16958">arXiv</a>
              /
              <a href="https://pku-epic.github.io/PartManip/" target="_blank">Project Page</a>
              /
              <a href="https://github.com/PKU-EPIC/PartManip" target="_blank">Code</a>
              <br>
              <em><strong>CVPR 2023</strong>, Accepted</em>
              <p></p>
              <p>We introduce a large-scale, part-based, cross-category object manipulation benchmark with tasks in realistic, vision-based settings. </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/E2E_3.png' width="190"></div>
                  <br>
                  <img src='images/E2E_3.png' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/hyperplane-lab/RLAfford?style=flat-square&logo=GitHub&color=yellow">

              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2209.12941">
                <papertitle><br>RLAfford: End-to-End Affordance Learning for Robotic Manipulation</papertitle>
              </a>
              <br>
              <a:focus><strong>Yiran Geng*</strong></a:focus>,
              <a:focus>Boshi An*</a:focus>,
              <a:focus>Haoran Geng</a:focus>,
              <a:focus href="https://cypypccpy.github.io/">Yuanpei Chen</a:focus>,
              <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>,
              <a:focus href="https://zsdonghao.github.io">Hao Dong</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2209.12941">arXiv</a>
              /
              <a href="https://sites.google.com/view/rlafford/">Project Page</a>
              /
              <a href="https://www.bilibili.com/video/BV1cM411b7ZD/?spm_id_from=444.41.list.card_archive.click">Video</a>
              /
              <a href="https://github.com/hyperplane-lab/RLAfford">Code</a>
              /
              <a href="https://github.com/GengYiran/Draw_PointCloud">Code (Renderer)</a>
              /
              <a href="https://github.com/boshi-an/SapienDataset">Dataset</a>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498641&idx=1&sn=06a3f2a314e43def32df5ee76e47fa55&chksm=fb1af784cc6d7e92852a53e994d57139d7beed0e379bf3ea7c5d66e4fe558b4a70d14f19832d&mpshare=1&scene=1&srcid=0125YlFVj1P4UMAt6V9CcKKq&sharer_sharetime=1674658986915&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd"
              >Media (CFCS)</a>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzA3OTE0MjQzMw==&mid=2651954123&idx=1&sn=d4f6814fc92e6e7ded87f74ffedf723c&chksm=845d3085b32ab9937babd49f0a048340a00e847666dc7126ef1b1b04b48974cae90e32134db1&mpshare=1&scene=1&srcid=0214KpmupUsYZkxuL6TboxV1&sharer_sharetime=1676344711865&sharer_shareid=8d02b18bc72b08d54fb4e28c3b683968#rd
              ">Media (PKU)</a></li>
              <br>
              <em><strong>ICRA 2023</strong>, Accepted</em>
              <p></p>
              <p>In this study, we take advantage of visual affordance by using the contact information generated during the RL training process to predict contact maps of interest. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/GraspNerf.png' width="190"></div>
                <img src='images/GraspNerf.png' width="190">
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-EPIC/GraspNeRF?style=flat-square&logo=GitHub&color=yellow">

              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2210.06575">
                <papertitle><br>GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</papertitle>
              </a>
              <br>
              <a:focus href="https://daiqy.github.io/">Qiyu Dai*</a:focus>,
              <a:focus>Yan Zhu*</a:focus>, 
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Ciyu Ruan</a:focus>,
              <a:focus href="https://www.researchgate.net/profile/Jiazhao-Zhang-2">Jiazhao Zhang</a:focus>,
              <a:focus href="https://hughw19.github.io/">He Wang</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2210.06575">arXiv</a>
              /
              <a href="https://pku-epic.github.io/GraspNeRF">Project Page</a>
              /
              <a href="https://github.com/PKU-EPIC/GraspNeRF">Code</a>
              <br>
                <em><strong>ICRA 2023</strong>, Accepted</em>
              <p></p>
              <p>
                We propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/GenDexGrasp.gif' width="190"></div>
                <br>
                <img src='images/GenDexGrasp.gif' width="190">
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/tengyu-liu/GenDexGrasp?style=flat-square&logo=GitHub&color=yellow">

                
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2210.00722/">
                <papertitle>GenDexGrasp: Generalizable Dexterous Grasping</papertitle>
              </a>
              <br>
              <a:focus>Puhao Li*</a:focus>,
              <a:focus>Tengyu Liu*</a:focus>,
              <a:focus>Yuyang Li</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus href="https://yzhu.io/">Yixin Zhu</a:focus>,
              <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>,
              <a:focus href="https://siyuanhuang.com/">Siyuan Huang</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2210.00722/">arXiv</a>
              /
              <a href="https://sites.google.com/view/gendexgrasp/">Project Page</a>
              /
              <a href="https://github.com/tengyu-liu/GenDexGrasp">Code</a>
              /
              <a href="https://sites.google.com/view/gendexgrasp/multidex">Dataset</a>
              /
              <a href="https://vimeo.com/753797406/1ef3563db3">Video</a>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzkzMjM4MzA3Mg==&mid=2247488703&idx=1&sn=40c893ebc4c09932e789fe598aeb1f02&chksm=c25dc4b0f52a4da66035fb9befbc2164cd2f53e47b1992050e02c04aae0a0eba1d13d890b8af&mpshare=1&scene=1&srcid=0331juUNZmWAwzo3x9pYqXch&sharer_sharetime=1680265050824&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd">Media</a>
              <br>
              <em><strong>ICRA 2023</strong>, Accepted</em>
              <p></p>
              <p>
                This paper introduces GenDexGrasp, a versatile dexterous grasping method that can generalize to unseen hands.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/myo_method2.jpeg' width="190"></div>
                  <br>
                <img src='images/myo_method2.jpeg' width="190">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/myochallenge">
                <papertitle>MyoChallenge: Learning contact-rich manipulation using a musculoskeletal hand</papertitle>
              </a>
              <br>
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Boshi An</a:focus>,
              <a:focus>Yifan Zhong</a:focus>,
              <a:focus>Jiaming Ji</a:focus>,
              <a:focus>Yuanpei Chen</a:focus>,
              <a:focus>Hao Dong</a:focus>,
              <a:focus>Yaodong Yang</a:focus>
              <br>
              <a href="https://proceedings.mlr.press/v220/caggiano23a/caggiano23a.pdf">Paper</a>
              /
              <a href="https://sites.google.com/view/myochallenge">Challenge Page</a>
              /
              <a href="https://github.com/PKU-MARL/MyoChallenge">Code</a>
              /
              <a href="pdf/DieRotation_NIPS22.pdf">Slides</a></li>
              /
              <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox">Talk</a></li>
              /
              <a href="images/awards/myochallenge_award.png">Award</a></li>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=Mzg2Njc1NjM2OA==&mid=2247494056&idx=1&sn=ad4a2b5195044bd4a73806c00641584b&chksm=ce475bd7f930d2c18344752b506df32daec23fdc7418265cb186961f9b550ec04981cfca95a7&mpshare=1&scene=1&srcid=0207iqqMmPfyBvV5g1RuPCdO&sharer_sharetime=1675771756722&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd
              ">Media (BIGAI)</a></li>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498719&idx=1&sn=8a6630cdd46a8eafccf213eec2b92b76&chksm=fb1af7cacc6d7edc8c56b0f938b0615d3dbdb1d032ad830d82b48934b7e29cffc011d47b42f8&mpshare=1&scene=1&srcid=0207rvp6Pj3EzGQyeZC7Kzcr&sharer_sharetime=1675771732370&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd
              ">Media (CFCS)</a></li>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzA4MTAzMzQ5NA==&mid=2650882501&idx=1&sn=f6cd654038af025123468e3a490cc13f&chksm=846ec1bcb31948aad69a103063d51d20be78ef2b6be4253af26e42469b9f0439609a3bf87b8e&mpshare=1&scene=1&srcid=0207NcfZnJPgvrt11Qc3txi1&sharer_sharetime=1675778883713&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd
              ">Media (PKU-EECS)</a></li>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzkzMjM4MzA3Mg==&mid=2247488531&idx=1&sn=0b1c0066c0efa6ef0c7421d4d8de3e28&chksm=c25dc41cf52a4d0a5ee0b69f6fbfa47affa203af036cc0ecc31a503f6e30b5b6772969fe73e7&mpshare=1&scene=1&srcid=02098BnGNZesAATeZd0US28z&sharer_sharetime=1675917111487&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd
              ">Media (PKU-IAI)</a></li>
              /
              <a href="https://mp.weixin.qq.com/s?__biz=MzA3OTE0MjQzMw==&mid=2651954123&idx=1&sn=d4f6814fc92e6e7ded87f74ffedf723c&chksm=845d3085b32ab9937babd49f0a048340a00e847666dc7126ef1b1b04b48974cae90e32134db1&mpshare=1&scene=1&srcid=0214KpmupUsYZkxuL6TboxV1&sharer_sharetime=1676344711865&sharer_shareid=8d02b18bc72b08d54fb4e28c3b683968#rd
              ">Media (PKU)</a></li>
              /
              <a href="http://zqb.cyol.com/html/2023-03/17/nw.D110000zgqnb_20230317_1-07.htm
              ">Media (China Youth Daily)</a></li>
              
              <br>
              <b>First Place in NeurIPS 2022 Challenge Track (1st in 340 submissions from 40 teams)</b>
              <!-- <br> -->
                <!-- <em><strong>NeurIPS 2023</strong></em> -->
              <p></p>
              <p>Reconfiguring a die to match desired goal orientations. This task require delicate coordination of various muscles to manipulate the die without dropping it. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/PLAfford.jpg' width="194"></div>
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <p></p>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" target="_blank">
                <papertitle>Learning Part-Aware Visual Actionable Affordance for 3D Articulated Object Manipulation</papertitle>
              </a>
              <br>
              <a:focus> Yuanchen Ju*</a>,
              <a:focus>Haoran Geng*</strong></a:focus>,
              <a:focus>Ming Yang</a:focus>*,
              <a:focus>Jiayi Chen</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a:focus>,
              <a:focus>Yaroslav Ponomarenko</a:focus>,
              <a:focus>Taewhan Kim</a:focus>,
              <a:focus>He Wang</a:focus>,
              <a:focus>Hao Dong</a:focus>
              <br>
              <a href="">Paper</a>
              /
              <a href="https://drive.google.com/file/d/19bPWbLLnsQTBU5xT64yRQ6HgbOjrZoKv/view?usp=sharing" target="_blank">Video</a>
              /
              <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">Workshop</a>
              
              <!-- <pre> -->
                <p id="PartManip" style="font:1px; display: none">
                @article{geng2023partmanip,
                  <br>
                  &emsp;title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
                  <br>
                  &emsp;author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
                  <br>
                  &emsp;journal={arXiv preprint arXiv:2303.16958},
                  <br>
                  &emsp;year={2023}
                  <br>
                }
            </p >
              <br>
              <em><strong>CVPR 2023 @ 3DVR</strong>, <b><font color='red'>Spotlight Presentation</font></b></em>
              <p></p>
              
              <p>We introduces Part-aware Affordance Learning methods. Our approach first learns a part 
                prior, subsequently generating an affordance map.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one"><br>
                <div class="two" id='malle_image'>
                  <img src='images/2022grasparl.gif' width="190"></div>
                <img src='images/2022grasparl.gif' width="190">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arXiv.org/abs/2203.02119">
                <papertitle>GraspARL: Dynamic Grasping via Adversarial Reinforcement Learning</papertitle>
              </a>
              <br>
              <a:focus href="https://tianhaowuhz.github.io/">Tianhao Wu*</a:focus>, 
              <a:focus href="https://fangweizhong.xyz/">Fangwei Zhong*</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a>,
              <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>,
              <a:focus href="https://cs.pku.edu.cn/info/1085/1331.htm">Yizhou Wang</a:focus>,
              <a:focus href="https://zsdonghao.github.io">Hao Dong</a:focus>
              <br>
              <a href="https://arXiv.org/abs/2203.02119">arXiv</a>
              <!-- <br>
              <em>Under Review</em> -->
              <p></p>
              <p>
                This study is the first attempt to formulate the dynamic grasping problem as a ‚Äúmove-and-grasp‚Äù game and use adversarial RL to train the grasping policy and object moving strategies jointly.
              </p>
            </td>
          </tr>
    
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <heading>Before 2022</heading>
        </tr>
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image'>
              <br>
              <img src='images/YINGCAI2.png' width="190"></div>
              <br>
            <img src='images/YINGCAI2.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start() {
              document.getElementById('malle_image').style.opacity = "1";
            }
            function malle_stop() {
              document.getElementById('malle_image').style.opacity = "0";
            }
            malle_stop()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="pdf/YINGCAI.pdf">
            <papertitle>Ministry of Education Talent Program Thesis (Physics Track)</papertitle>
          </a>
          <br>
          <a:focus><strong>Yiran Geng</strong></a:focus>, 
          <a:focus>Haoran Geng</a:focus>, 
          <a:focus>Xintian Dong</a:focus>,
          <a:focus>Yue Meng</a:focus>,
          <a:focus>Xujv Sun</a:focus>,
          <a:focus>Houpu Niu</a:focus>
          <br>
          <a href=pdf/YINGCAI.pdf>PDF</a>
          <br>
          <em>Selected as the Outstanding Thesis of the National Physics Forum 2018</em>
          <p>
            During my high school years, I was selected for the Ministry of Education Talent Program and conducted physics research at Nankai University and presenting my thesis at the National Physics Forum 2018.
        </td>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <br>
              <heading>Experience</heading>
          <br>
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/MIT.png", width="100"></td>
            <td width="90%" valign="center">
              <strong>Massachusetts Institute of Technology (MIT) </strong>
              <br> 2023.01 - 2023.10
              <br> <strong>Visiting Researcher</strong>
              <br> Research Advisor: Prof. <a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a> and Prof. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a> 
            </td>
          </tr>  
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/Bigai.png", width="100"></td>
            <td width="90%" valign="center">
              <strong>Beijing Institute for General Artificial Intelligence (BIGAI) </strong>
              <br> 2022.05 - 2023.06
              <br> <strong>Research Intern</strong>
              <br> Research Advisor: Prof. <a href="https://www.yangyaodong.com/">Yaodong Yang</a> 
              <br> Academic Advisor: Prof. <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
            </td>
          </tr>
          <tr>
              <br>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/PKU.png", width="100"></td>
              <td width="90%" valign="center">
                <strong>Turing Class, Peking University (PKU)</strong>
                <br> 2020.09 - Present
                <br> <strong>Undergraduate Student</strong>
                <br> Research Advisor: Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>
                <br> Academic Advisor: Prof. <a href="https://zhenjiang888.github.io/">Zhenjiang Hu</a>
              </td>
          </tr>
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
            <heading>Service and Teaching</heading>
            <br><br>
          <td style="padding:0px;width:100%;vertical-align:middle">
            <p>
              <li>Reviewer: The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2024)</li>
            </p>
            <p>
              <li>Reviewer: 2024 IEEE Robotics and Automation Letters (RAL 2024)</li>
            </p>
            <p>
              <li>Reviewer: 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)</li>
            </p>
            <p>
              <li>Program Committee: The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)</li>
            </p>
            <p>
              <li>Reviewer: Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)</li>
            </p>
            <p>
              <li>Reviewer: International Conference on Computer Vision (ICCV 2023)</li>
            </p>
            <p>
              <li>Reviewer: The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2023)</li>
            </p>
            <p>
              <li>Teaching Assistant: Probability Theory and Statistics (A) (Spring 2023)</li>
            </p>
            <p>
              <li>Teaching Assistant: Study and Practice on Topics of Frontier Computing (II) (Spring 2023) [<a href="https://github.com/GengYiran/RL-Robot-Assignment#pre-requisite-knowledge">Assignment</a>]
              </li>
            </p>
            <p>
              <li>Volunteer: Conference on Web and Internet Economics (WINE 2020)</li>
            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
              <heading>Recent Talks</heading>
            <br><br>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <p>
                <li>[August 20] Student Representative Speech at the Annual School Assembly of EECS
                [<a href="pdf\Speech_2023_Assembly.pdf">Manuscript</a>]
                </li>
             </p>
              <p>
                 <li>[May 27] "RLAfford: End-to-End Affordance Learning for Robotic Manipulation" at Turing Student Forum (<strong>Outstanding Presentation Award</strong>)
                 [<a href="https://www.bilibili.com/video/BV1pM4y1i77m/?spm_id_from=333.999.0.0&vd_source=29e736b58716f318ce554795bbbe6149">Video</a>]
                 </li>
              </p>
              <p>
                <li>[May 13] "Agent Manipulation Skill Learning through Reinforcement Learning" at School of EECS Research Exhibition (<strong>Outstanding Presentation Award</strong>)
                  [<a href="pdf/eecs_poster.pdf">Poster</a>]
                </li>
              </p>
              <p>
                <li>[Mar 4] "Research Experience Sharing - PKU Undergraduates' Perspective" for Tong Class
                </li>
              </p>
              <p>
                <li>[Jan 30] "Recent Advances in Model Predictive Control" at BIGAI
                  [<a href="https://meeting.tencent.com/user-center/shared-record-info?id=e8f0edf6-b2c0-4f17-ad7d-84091ef2c327&hide_more_btn=true&from=3">Video</a>]
                </li>
              </p>
              <p>
                <li>[Dec 7 (UTC time)] Winner presentation on <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox" target="_blank">NeurIPS 2022 MyoChallenge workshop</a>
                  [<a href="https://neurips.cc/virtual/2022/competition/50098">Link</a>
                  /
                  <a href="pdf/DieRotation_NIPS22.pdf">Slides</a>]
                </li>
              </p>
              <P>
                <li>[TBD] "Agent Manipulation Skill Learning through Reinforcement Learning" at <a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm" target="_blank">CS Peer Talks</a> at CFCS 
                  [<a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm">Link</a>]
                </li>
              </P>
              <P>
                <li>[TBD] "End-to-End Affordance Learning for Robotic Manipulation" at <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498147&idx=1&sn=9039ff0cf8ea6f0a6654b4ea44b586bf&chksm=fb1af5b6cc6d7ca0328ab3a7dd20dff998d3eba7d472fd1b403e8ac6dc34fdffed20c0e37fe0&mpshare=1&scene=1&srcid=1129ZzNYSx7cW5qkvJYwLwdY&sharer_sharetime=1669702563247&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd" target="_blank">CFCS Frontier Storm</a> at CFCS 
                  [<a href="pdf/poster.pdf">Poster</a>]
                </li>
              </P>
            </td>
          </tr>
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <br>
                <heading>Selected Awards and Honors</heading>
                <br>
                <br>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <p>
                  <li><span style="background-color: lightyellow;">2023: May Fourth Scholarship (<strong>Highest-level Scholarship</strong> for Peking University, 125/65k+, ¬•12000 RMB)</span></li>
                </p>
                <p>
                  <li>2023: Academic Innovation Award of Peking University</li>
                </p>
                <p>
                  <li>2023: Undergraduate Research Outstanding Scholarship, President's Fund (¬•4000 RMB)</li>
                </p>
                <p>
                  <li>2023: Merit Student of Peking University</li>
                </p>
                <p>
                  <li>2023: Peking University Dean's Scholarship (¬•5000 RMB)</li>
                </p>
                <p>
                  <li>2023: Second Place in ICCV 2023 Challenge Track ü•à</li>
                </p>
                <p>
                  <li>2023: Peking University Education Foundation Scholarship (¬•4500 RMB)</li>
                </p>
                <p>
                  <li>2023: Outstanding Research Presentation Award in Turing Student Research Forum</li>
                </p>
                <p>
                  <li>2023: Peking University Dean's Scholarship (¬•5000 RMB)</li>
                </p>
                <p>
                  <li>2023: Peking University School of EECS Outstanding Research Presentation Award</li>
                </p>
                <p>
                  <li>2022: Beijing Institute for General Artificial Intelligence (BIGAI) Outstanding Contribution Award (¬•6500 RMB)</li>
                </p>
                <p>
                  <li>2022: Center on Frontiers of Computing Studies (CFCS) Outstanding Student</li>
                </p>
                <p>
                  <li>2022: John Hopcroft Scholarship (¬•3000 RMB)</li>
                </p>
                <p>
                  <li><span style="background-color: lightyellow;">2022: <strong>First Place</strong> in NeurIPS 2022 Challenge Track (‚Ç¨3000) üèÖ</span></li>
                </p>
                <p>
                  <li>2022: Peking University Research Excellence Award</li>
                </p>
                <p>
                  <li>2022: Peking University Dean's Scholarship (¬•15000 RMB)</li>
                </p>
                <p>
                  <li><span style="background-color: lightyellow;">2021: SenseTime Scholarship (<strong>Youngest winner</strong>, 30/year in China, ¬•20000 RMB)</span></li>
                </p>
                <p>
                  <li>2021: Peking University Study Excellence Award</li>
                </p>
                <p>
                  <li>2021: Peking University Qinjin Scholarship (¬•10000 RMB)</li>
                </p>
                <p>
                  <li>2021: John Hopcroft Scholarship (¬•3000 RMB)</li>
                </p>
                <p>
                  <li>2021: Ministry of Education Top Talent Program Scholarship (¬•1000 RMB)</li>
                </p>
                <p>
                  <li><span style="background-color: lightyellow;">2019: Chinese Physics Olympiad (CPHO) <strong>Gold Medalist</strong> üèÖ</span></li>
                </p>
                <p>
                  <li><span style="background-color: lightyellow;">2017: Rank 1st/100k in National High School Entrance Examination (Â§©Ê¥•Â∏Ç‰∏≠ËÄÉË£∏ÂàÜÁ¨¨‰∏ÄÂêç) üèÖ</span></li>
                </p>
              </td>
            </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    This template is a modification to <a href="https://rochelleni.github.io/">Jiayi Ni</a>üíó's website and <a href="https://jonbarron.info/">Jon Barron</a>'s website. 
                </p>
              </td>
            </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>
</html>
