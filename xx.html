<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <script type="text/javascript" src="js/js_func.js"></script>
  <title>Baifeng Shi</title>

  <meta name="author" content="Baifeng Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.jpg">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
  <td style="padding:0px">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:63%;vertical-align:middle">
        <p style="text-align:center">
          <name>Baifeng Shi</name>
        </p>
        <p>I am a first-year Ph.D. student advised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> at <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab</a>. Previously, I graduated from Peking University with a B.S. degree in computer science. I’ve had the fortune to work with Prof. <a href="http://www.muyadong.com/index.html">Yadong Mu</a>  at Peking University, and with Dr. <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a> and Dr. <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a> at Microsoft Research Asia.
        </p>
        <p>
          My primary interests lie in how to formulate the process of visual perception on both the (top-down) conceptual and (bottom-up) computational level, with the goal of learning with minimal labels (e.g. weak/self supervision) and making robust decisions across environments and tasks (e.g. domain generalization, multi-task) like a human being.
        </p>
        <p style="text-align:center">
          <a href="mailto:baifeng_shi@berkeley.edu">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?hl=en&user=LBEIm8gAAAAJ">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/bfshi">Github</a> &nbsp/&nbsp
          <a href="files/cv.pdf">CV</a> &nbsp/&nbsp
          <a href="files/wechat.jpg">WeChat</a>
        </p>
      </td>
      <td style="padding:2.5%;width:30%;max-width:30%">
        <a href="images/avatar.jpg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading><center>Publications</center></heading>

      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle">
        <img src='images/VARS.png' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Visual Attention Emerges from Recurrent Sparse Reconstruction</papertitle>
        <br>
        <strong>Baifeng Shi</strong>,
        <a href="http://people.csail.mit.edu/yalesong/home/">Yale Song</a>,
        <a href="https://neelj.com/">Neel Joshi</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://xinw.ai/">Xin Wang</a>
        <br>
        <em>ICML</em>, 2022
        <br>
        <a href="javascript:toggleblock('VARS_abs')">abstract</a> /
        <a href="https://arxiv.org/abs/2204.10962">pdf</a> /
        <a href="https://github.com/bfshi/VARS">code</a>

        <p align="justify"> <i id="VARS_abs">Visual attention helps achieve robust perception under noise, corruption, and distribution shifts in human vision, which are areas where modern neural networks still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity. Related features are grouped together via recurrent connections between neurons, with salient objects emerging via sparse regularization. VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time. Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of "templates" encoding underlying patterns of data. We show that self-attention is a special case of VARS with a single-step optimization and no sparsity constraint. VARS can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks.</i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:40%;vertical-align:middle">
        <img src='images/ICCV2021_SSAD_OSAD.jpg' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Temporal Action Detection with Multi-level Supervision</papertitle>
        <br>
        <strong>Baifeng Shi</strong>,
        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,
        <a href="https://www.cc.gatech.edu/~judy/">Judy Hoffman</a>,
        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>
        <br>
        <em>ICCV</em>, 2021
        <br>
        <a href="javascript:toggleblock('ICCV2021_SSAD_OSAD_abs')">abstract</a> /
        <a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Temporal_Action_Detection_With_Multi-Level_Supervision_ICCV_2021_paper.pdf">pdf</a>

        <p align="justify"> <i id="ICCV2021_SSAD_OSAD_abs">Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification literature. Identifying that the main source of error is action incompleteness (ie, missing parts of actions), we alleviate it by designing an unsupervised foreground attention (UFA) module utilizing the conditional independence between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. To overcome the accompanying action-context confusion problem in OSAD baselines, an information bottleneck (IB) is designed to suppress the scene information in non-action frames while preserving the action information. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.</i></p>
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding:10px;width:40%;vertical-align:middle">
        <img src='images/NeurIPS2020_ARML.png' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Auxiliary Task Reweighting for Minimum-data Learning</papertitle>
        <br>
        <strong>Baifeng Shi</strong>,
        <a href="https://www.cc.gatech.edu/~judy/">Judy Hoffman</a>,
        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>
        <br>
        <em>NeurIPS</em>, 2020
        <br>
        <a href="javascript:toggleblock('NeurIPS2020_ARML_abs')">abstract</a> /
        <a href="https://arxiv.org/abs/2010.08244">pdf</a> /
        <a href="https://github.com/bfshi/ARML_Auxiliary_Task_Reweighting">code</a> /
        <a href="https://sites.google.com/view/auxiliary-task-reweighting/home">project page</a> /
        <a href="https://drive.google.com/file/d/1UiNHBztUVJbhvDJTmFww7v4ZPSdXD4DM/view">slides</a> /
        <a href="https://www.youtube.com/watch?v=B2i6z6HefGw">video</a>

        <p align="justify"> <i id="NeurIPS2020_ARML_abs">Auxiliary tasks are widely used to address the lack of data by providing additional supervision in semi/self-supervised learning, transfer learning, reinforcement learning, etc. Assigning the importance weights for different auxiliary tasks remains a crucial, and largely understudied, research question. In this work, we formulate the problem as minimizing the information required to learn the main task. With the key insight that information required for inference can be reduced by a good prior, we propose an algorithm to automatically reweight auxiliary tasks so that the surrogate prior given by the weighted likelihood of auxiliary tasks is optimized. We further reduce the optimization problem into minimizing the l2 distance between main/auxliary task gradients by adopting tools including Langevin dynamics, Fisher divergence, etc. Our algorithm finds the optimal weights and minimizes the required data under various settings, as supported by both theoretical guarantees and experimental observations.</i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:40%;vertical-align:middle">
        <img src='images/ECCV2020.png' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Weakly-supervised Action Localization with Expectation-Maximization Multi-instance Learning</papertitle>
        <br>

        <a href="https://scholar.google.com/citations?user=kqVZxpYAAAAJ&hl=en">Zhekun Luo</a>,
        <a href="https://www.devinguillory.com/">Devin Guillory</a>,
        <strong>Baifeng Shi</strong>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=BENt-uEAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate">Wei Ke</a>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=0IKavloAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate">Fang Wan</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>
        <br>
        <em>ECCV</em>, 2020
        <br>
        <a href="https://arxiv.org/abs/2004.00163">pdf</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:40%;vertical-align:middle">
        <img src='images/ICML2020_InfoDrop.jpg' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Informative Dropout for Robust Representation Learning: A Shape-bias Perspective</papertitle>
        <br>
        <strong>Baifeng Shi*</strong>,
        <a href="https://zdhnarsil.github.io/">Dinghuai Zhang*</a>,
        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,
        <a href="https://scholar.google.co.uk/citations?user=a2sHceIAAAAJ&hl=en">Zhanxing Zhu</a>,
        <a href="http://www.muyadong.com/index.html">Yadong Mu</a>,
        <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
        <br>
        <em>ICML</em>, 2020
        <br>
        <a href="javascript:toggleblock('ICML2020_InfoDrop_abs')">abstract</a> /
        <a href="https://arxiv.org/abs/2008.04254">pdf</a> /
        <a href="https://github.com/bfshi/InfoDrop">code</a> /
        <a href="https://icml.cc/virtual/2020/poster/5839">video</a> /
        <a href="https://zhuanlan.zhihu.com/p/197929813">知乎</a>

        <p align="justify"> <i id="ICML2020_InfoDrop_abs">In this work, we attempt at improving various kinds of robustness universally by alleviating CNN’s texture bias. With inspiration from the human visual system, we propose to discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. We observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation).</i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:40%;vertical-align:middle">
        <img src='images/CVPR2020_DGAM.png' width="100%">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle">
        <papertitle>Weakly-supervised Action Localization by Generative Attention Modeling</papertitle>
        <br>
        <strong>Baifeng Shi</strong>,
        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,
        <a href="http://www.muyadong.com/index.html">Yadong Mu</a>,
        <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
        <br>
        <em>CVPR</em>, 2020
        <br>
        <a href="javascript:toggleblock('CVPR2020_DGAM_abs')">abstract</a> /
        <a href="http://arxiv.org/abs/2003.12424">pdf</a> /
        <a href="https://github.com/bfshi/DGAM-Weakly-Supervised-Action-Localization">code</a> /

        <p align="justify"> <i id="CVPR2020_DGAM_abs">Regular weakly-supervised action localization methods mostly follows the “localizing by classifying” paradigm, which results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves. With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model is learned to model the class-agnostic likelihood of each frame given the attention. By maximizing the conditional probability, we obtain the MAP estimation of the attention.</i></p>
        <p></p>
      </td>
    </tr>




    </tbody></table>

    <table class="center"><tbody>
    <tr>
      <td>
        <heading>Experience</heading>
      </td>
    </tr>
    </tbody></table>
    <table width="50%" align="center" border="0" cellpadding="10"><tbody>
    <tr>
      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle"><img src="images/ucb.png", width="90%"></td>
      <td width="80%" valign="center" >
        <b>University of California, Berkeley</b>
        <br> 2021.09 - Present
        <br>
        <br> <b>Ph.D. Student in Computer Science</b>
        <br> Advisor: Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
      </td>
    </tr>
    <tr>
      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle;"><img src="images/microsoft.png", width="90%"></td>
      <td width="80%" valign="center">
        <b>Microsoft Research Asia</b>
        <br> 2019.09 - 2021.02
        <br>
        <br> <b>Research Intern</b>
        <br> Advisor: Dr. <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a> and Dr. <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
      </td>
    </tr>
    <tr>
      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle"><img src="images/peking_university.png", width="90%"></td>
      <td width="80%" valign="center">
        <b>Peking University</b>
        <br> 2017.09 - 2021.06
        <br>
        <br> <b>B.S. in Computer Science</b>, Turing Class
        <br> Advisor: Prof. <a href="http://www.muyadong.com/index.html">Yadong Mu</a>
      </td>
    </tr>


    </tbody></table>

<!--    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--    <tr>-->
<!--      <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--        <heading><center>Miscellaneous</center></heading>-->
<!--        <p>-->
<!--          I was quite into competitive programming&#128187; and used to compete in Olympiad in Informatics and <a href="https://icpc.global/">International Collegiate Programming Contest (ICPC)</a>&#127941;.-->
<!--          Now I also train high school students back at home. Solving those brain teasers is always fun!-->
<!--          <br> Away from computers, I enjoy learning foreign languages and watching musicals&#127917;&#127926;.-->
<!--        </p>-->
<!--      </td>-->
<!--    </tr>-->
<!--    </tbody></table>-->

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          I stole the template from <a href="https://yijiaweng.github.io/">Yijia Weng</a>.
          <br> Last updated: Apr 26, 2022
        </p>
      </td>
    </tr>
    </tbody></table>
  </td>
</tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideblock('VARS_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ICCV2021_SSAD_OSAD_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('NeurIPS2020_ARML_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ICML2020_InfoDrop_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('CVPR2020_DGAM_abs');
</script>
</body>

</html>
Footer
