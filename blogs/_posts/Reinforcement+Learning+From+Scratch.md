# Reinforcement Learning From Scratch
##### Jiayi, Yiran, Yitao

(2024.1.21 continually updating ...)
# Overview
- Books
	- Theory of Deep Learning
	- RL: an Introduction (Sutton)
- Open source code (strongly recommend!)
	- Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π† https://zh.d2l.ai/
	- Âä®ÊâãÂ≠¶Âº∫ÂåñÂ≠¶‰π† HRL
		- Book https://hrl.boyuai.com/
		- Code https://github.com/boyu-ai/Hands-on-RL

# Pre-requisites
- create conda environment
- pip install packages
- check GitHub issues
# RL: an Introduction (Sutton)
Reference materials below do not need to be read in full! They are to serve your learning, not to add extra pressure ~
- Free Online Book
http://incompleteideas.net/book/RLbook2020.pdf
- Official Slides
https://drive.google.com/drive/folders/1cMJWR90IkMxWngWpjOtD-qdLS_a7KiYL
- Exercise Solutions
Send in your solutions for a chapter, get the official ones back @richsutton.com
https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions
https://github.com/brynhayder/reinforcement_learning_an_introduction/tree/master/exercises

- Notes
https://github.com/brynhayder/reinforcement_learning_an_introduction/blob/master/notes/notes.pdf

- Demo
https://cs.stanford.edu/people/karpathy/reinforcejs/index.html

- Code
Official Code in C and Lisp http://incompleteideas.net/book/code/code2nd.html
Hands-on Reinforcement Learning https://hrl.boyuai.com/

- Courses
https://www.davidsilver.uk/teaching/
https://web.stanford.edu/class/cs234/modules.html

- Key points
	- Q learning
	    - [https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf) p35
	    - [https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning](https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning)
	        - The reason that Q-learning is off-policy is that it updates its Q-values using the Q-value of the next state ùë†‚Äô and the _greedy action_ ùëé‚Äô. In other words, it estimates the _return_ (total discounted future reward) for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy.
	- Relationship Between DP and TD
	    - [https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf) p41
	- How is TD derived? Relationship between TD and DP, MC

# Playing Atari with DQN

DQN by DeepMind showcases for the first time a reinforcement learning algorithm that directly utilizes a convolutional neural network to handle pixel input and play various Atari games. DQN serves as the foundation for deep reinforcement learning, but mastering this algorithm requires basic familiarity with reinforcement learning and neural networks. This step-by-step guidance is to equip you with the fundamentals.

Part A and Part B are crucial for understanding policies and environments in reinforcement learning. Part C is for neural network implementation. It's strongly recommended that you go through each line and complete all the tasks listed. While it might take some time, trust me, you would reap what you sow. 

These sections can be tackled in any order. It's more natural to start with Part A, but if you're not confident in your coding skills, starting with Part B might be easier for you ~ To help you assess your comprehension, I've included a set of self-check questions for each individual task in Part B.

Hope you enjoy! :)

#### Part A: Algorithm Implementation
- Based on pseudocode from Sutton, implement Policy Iteration (including Policy Evaluation and Policy Improvement), Value Iteration, Sarsa, n-step Sarsa, and Q-learning algorithms.
	- If implementing feels a bit challenging, focus on the core and skip less critical parts. 
- Read Chapter 4 (DP) and Chapter 5 (TD) of HRL https://hrl.boyuai.com/ and compare the code you write with the reference code.
	- In addition to correctness of code logic, you can also pay attention to how the reference code encapsulates functions/classes, and names variables.


#### Part B: Getting familiar with Environment
- Run the reference code of HRL in provided Cliff Walking and Frozen Lake environments. Remember to execute the provided print functions and visualization tools to intuitively understand the interaction between policies and environments. Feel free to tweak the code and experiment with it as you like.
	- Compare HRL's implementation of Cliff Walking environment https://hrl.boyuai.com/chapter/1/Âä®ÊÄÅËßÑÂàíÁÆóÊ≥ï/#42-ÊÇ¨Â¥ñÊº´Ê≠•ÁéØÂ¢É with Openai Gym's version https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py
		- Explain this line: `P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]`
		- Why is `P` unnecessary for TD policies?
	- https://hrl.boyuai.com/chapter/1/Âä®ÊÄÅËßÑÂàíÁÆóÊ≥ï/#45-ÂÜ∞ÊπñÁéØÂ¢É For Frozen Lake environment, make sure you read through the original Openai Gym's code https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py and document https://gymnasium.farama.org/environments/toy_text/frozen_lake/, and understand how Dynamic Programming policies you implemented interact with Frozen Lake Env.
		- How to create a default map of size 8x8? How to create a random one? A self-defined one?
		- What would be the result of  `print(env.P[6])`(for map size 4x4)? Read the code and think yourself, and then run a check.
		- What would be the result of  `print(env.P[15])`(for map size 4x4)? What makes the difference? (refer to `frozen_lake.py`)
		- Is the default environment set to be slippery or not? How to explicitly set it?
- Write a simplest environment and embed it into PPO (or any algorithm) in Stable Baselines 3
	- References you might find helpful:
		1. Example code of PPO: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example
		2. `custom_gym_env.ipynb`: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb
		3. Using Custom Environments: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html
		4. Vectorized Environments: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html
		5. `stable_baselines3.common.vec_env` & `stable_baselines3.common.envs`
- Run arbitrary OpenAI Gym's classic environment using Stable Baselines 3 or any algorithm (DP, TD, etc).

#### Part C: Neural Network Implementation
Through part A and B, we have become thoroughly familiar with the interactive relationship between policies and environments in reinforcement learning. Before delving into deep reinforcement learning, let's first acquaint ourselves with the basic implementation of neural networks.

- Write a basic neural network using PyTorch, including `__init__` and `forward` functions, a loss function and an optimizer. Data-loading, training and testing processes should all be included. 
	- You can refer to PyTorch tutorial for neural networks https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html
	- There are plenty of other tutorials with detailed examples available, but I won't post it here to encourage you to give it a try yourself!
- CS231n hw1 Q4: Two-Layer Neural Network https://cs231n.github.io/assignments2023/assignment1/#q4-two-layer-neural-network

Multilayer Perceptrons (MLP) are enough for a simpler environment like Cart Pole, with non-image states as input (coordinates and velocity of the car). But unfortunately, your evil mentor wishes you to implement DQN in the classic Atari environment, where convolutional layers become a necessity in your network structure to extract features from images. So it wouldn't hurt to learn some knowledge about convolutional neural networks. Think positively; this might be preparing for your future escape to computer vision:)

- CS231n notes https://cs231n.github.io/convolutional-networks/
- CS231n hw2 Q4: CNN https://cs231n.github.io/assignments2023/assignment2/#q4-convolutional-neural-networks
#### Part D: DQN
- Paper
	- Human-level control through deep reinforcement learning
	- Playing atari with deep reinforcement learning
- Video Demo
	- Google DeepMind's Deep Q-learning playing Atari Breakout! https://www.youtube.com/watch?v=V1eYniJ0Rnk
- PyTorch Tutorial
	- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
- CS285 hw3: 2 Deep Q-Learning
	- Handout https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/homeworks/hw3.pdf
	- Code https://github.com/berkeleydeeprlcourse/homework_fall2023/tree/main/hw3

#### Part E: Atari
- Paper
	- The arcade learning environment: An evaluation platform for general agents
	- Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents
- Openai Gym
	- https://gymnasium.farama.org/environments/atari/

#### Finallly!
- Implement DQN algorithm in Atari environment.

#### Oops... not yet finished!
Be wary of non-breaking bugs! Check out openai's blog and code after your own implementation.
- Blog https://openai.com/research/openai-baselines-dqn
- Code https://github.com/openai/baselines/tree/master/baselines/deepq
- Results https://github.com/openai/baselines-results/blob/master/dqn_results.ipynb

# Alphago Zero

After comprehending the interrelations among Dynamic Programming (DP), Monte Carlo (MC), and Temporal Difference (TD) through the first 8 chapters of Sutton's book, you've grasped the essence of reinforcement learning. Then, you would realize that all algorithms in the entire field of sequential control are variations of the algorithms you already know, lying somewhere between Monte Carlo and Dynamic Programming. (Except for policy gradient methods and imitation learning, which approximate the optimal policy directly and do not need to form an approximate value function). Now it's time to read AlphaGo Zero to personally experience this point. You would be amazed at how Monte Carlo, tree search, and neural networks, those things already familiar to you, can be combined.

#### Notes from Yitao
1. MCTS is the manifestation of Monte Carlo in trees, which can be understood as Monte Carlo plus multi-armed bandits, where each choice at a state is akin to pulling a lever on a slot machine. Although many people equate multi-armed bandits with one-step RL, they lead to different algorithmic development paths from multi-step sequential RL. Algorithms for multi-armed bandits can be ingeniously designed, but if there are many subsequent steps and it's uncertain when they end, many intricate algorithms may not work out, and only simple ideas can scale up quickly (the ease of scaling up for TD is also why TD is more commonly used compared to Monte Carlo).

2. People prefer using TD in scenarios where the state-action space is large. The efficiency of Monte Carlo in Go is higher than TD because Go has a more structured action space, where each move is fundamentally consistent across layers. At such times, the limitation of TD lies in its bootstrapping; the current estimate of state value depends on the estimate of the next state. If the estimate for the next step is inaccurate it leads to significant bias, whereas Monte Carlo has theoretical guarantees of being unbiased.